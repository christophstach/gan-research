trainer:
  # gpus: 1,
  gpus:
  distributed_backend: ddp
  accumulate_grad_batches: 1
  profiler: False
  max_epochs: 500
  fast_dev_run: False
  num_sanity_val_steps: 0
  early_stop_callback: False